import json
import matplotlib.pyplot as plt
import pandas as pd

# Leer las mÃ©tricas del archivo JSONL
metrics = []
with open('logs/bipartite-ncl-link-prediction/run_20250818_203340/metrics.jsonl', 'r') as f:
    for line in f:
        metrics.append(json.loads(line))

print("=== ANÃLISIS COMPARATIVO DE RESULTADOS ===")
print("Entrenamiento con 200 Ã©pocas vs entrenamiento anterior (100 Ã©pocas)")

# Extraer mÃ©tricas finales
final_metrics = metrics[-1]

print(f"\nğŸ”¥ MEJORAS SIGNIFICATIVAS LOGRADAS:")
print(f"ğŸ“ˆ AUC-ROC Test: 0.5738 â†’ {final_metrics['final_test_auc_mean']:.4f} (+{final_metrics['final_test_auc_mean']-0.5738:.4f})")
print(f"ğŸ“ˆ Hits@10:     0.85%  â†’ {final_metrics['final_test_hits@10_mean']*100:.2f}% (+{(final_metrics['final_test_hits@10_mean']-0.0085)*100:.2f}%)")
print(f"ğŸ“ˆ Hits@50:     3.81%  â†’ {final_metrics['final_test_hits@50_mean']*100:.2f}% (+{(final_metrics['final_test_hits@50_mean']-0.0381)*100:.2f}%)")
print(f"ğŸ“ˆ Hits@100:    7.51%  â†’ {final_metrics['final_test_hits@100_mean']*100:.2f}% (+{(final_metrics['final_test_hits@100_mean']-0.0751)*100:.2f}%)")

print(f"\n=== MÃ‰TRICAS DETALLADAS (200 Ã‰POCAS) ===")
print(f"ğŸ¯ AUC-ROC Test:     {final_metrics['final_test_auc_mean']:.4f}")
print(f"ğŸ¯ Average Precision: {final_metrics['final_test_ap_mean']:.4f}")
print(f"ğŸ¯ Precision:        {final_metrics['final_test_precision_mean']:.4f}")
print(f"ğŸ¯ Recall:           {final_metrics['final_test_recall_mean']:.4f}")
print(f"ğŸ¯ F1-Score:         {final_metrics['final_test_f1_mean']:.4f}")

print(f"\n=== MÃ‰TRICAS DE RANKING (Hits@K) ===")
print(f"ğŸ† Hits@10:  {final_metrics['final_test_hits@10_mean']*100:.2f}%")
print(f"ğŸ† Hits@50:  {final_metrics['final_test_hits@50_mean']*100:.2f}%")
print(f"ğŸ† Hits@100: {final_metrics['final_test_hits@100_mean']*100:.2f}%")

print(f"\n=== ANÃLISIS DE VALIDACIÃ“N ===")
print(f"ğŸ“Š AUC-ROC Val:      {final_metrics['final_val_auc_mean']:.4f}")
print(f"ğŸ“Š Hits@10 Val:     {final_metrics['final_val_hits@10_mean']*100:.2f}%")
print(f"ğŸ“Š Hits@100 Val:    {final_metrics['final_val_hits@100_mean']*100:.2f}%")

# Analizar la evoluciÃ³n del loss durante entrenamiento
train_losses = []
epochs = []
for metric in metrics:
    if 'tbgrl_train_loss' in metric:
        train_losses.append(metric['tbgrl_train_loss'])
        epochs.append(metric['epoch'])

print(f"\n=== EVOLUCIÃ“N DEL TRAINING LOSS ===")
print(f"ğŸ”¸ Loss inicial (Ã‰poca 1):   {train_losses[0]:.4f}")
print(f"ğŸ”¸ Loss final (Ã‰poca 200):   {train_losses[-1]:.4f}")
print(f"ğŸ”¸ Loss mÃ­nimo alcanzado:    {min(train_losses):.4f} (Ã‰poca {epochs[train_losses.index(min(train_losses))]})")
print(f"ğŸ”¸ Diferencia total:         {train_losses[-1] - train_losses[0]:.4f}")

# Identificar fases del entrenamiento
print(f"\n=== FASES DEL ENTRENAMIENTO ===")
# Fase 1: Ã‰pocas 1-50
phase1_avg = sum(train_losses[:50]) / 50
print(f"ğŸ”¸ Fase 1 (1-50):    Promedio = {phase1_avg:.4f}")

# Fase 2: Ã‰pocas 51-100  
if len(train_losses) >= 100:
    phase2_avg = sum(train_losses[50:100]) / 50
    print(f"ğŸ”¸ Fase 2 (51-100):  Promedio = {phase2_avg:.4f}")

# Fase 3: Ã‰pocas 101-150
if len(train_losses) >= 150:
    phase3_avg = sum(train_losses[100:150]) / 50
    print(f"ğŸ”¸ Fase 3 (101-150): Promedio = {phase3_avg:.4f}")

# Fase 4: Ã‰pocas 151-200
if len(train_losses) >= 200:
    phase4_avg = sum(train_losses[150:200]) / 50
    print(f"ğŸ”¸ Fase 4 (151-200): Promedio = {phase4_avg:.4f}")

print(f"\n=== DIAGNÃ“STICO DE CONVERGENCIA ===")
last_20_losses = train_losses[-20:]
loss_std = pd.Series(last_20_losses).std()
print(f"ğŸ”¸ DesviaciÃ³n estÃ¡ndar Ãºltimas 20 Ã©pocas: {loss_std:.4f}")
if loss_std < 0.01:
    print("âœ… CONVERGENCIA: El modelo ha convergido (baja variabilidad)")
elif loss_std < 0.02:
    print("âš¡ CONVERGIENDO: El modelo estÃ¡ cerca de converger")
else:
    print("ğŸ”„ ENTRENANDO: El modelo aÃºn estÃ¡ aprendiendo")

print(f"\n=== COMPARACIÃ“N GENERAL ===")
improvement_auc = final_metrics['final_test_auc_mean'] - 0.5738
improvement_hits10 = final_metrics['final_test_hits@10_mean'] - 0.0085
improvement_hits100 = final_metrics['final_test_hits@100_mean'] - 0.0751

print(f"ğŸš€ MEJORA EN AUC:     {improvement_auc:+.4f} ({improvement_auc/0.5738*100:+.1f}%)")
print(f"ğŸš€ MEJORA EN HITS@10: {improvement_hits10:+.4f} ({improvement_hits10/0.0085*100:+.1f}%)")
print(f"ğŸš€ MEJORA EN HITS@100:{improvement_hits100:+.4f} ({improvement_hits100/0.0751*100:+.1f}%)")

if improvement_auc > 0.01:
    print("ğŸ‰ EXCELENTE: Mejora significativa en AUC")
elif improvement_auc > 0.005:
    print("ğŸ‘ BUENO: Mejora notable en AUC")
else:
    print("ğŸ“Š LEVE: Mejora marginal en AUC")

print(f"\n=== PRÃ“XIMOS PASOS RECOMENDADOS ===")
if final_metrics['final_test_auc_mean'] > 0.65:
    print("âœ¨ Excelente rendimiento! Considera:")
    print("  - Probar learning rates mÃ¡s bajos (0.001)")
    print("  - Aumentar dimensiones de embedding (256)")
    print("  - Evaluar en otros datasets")
elif final_metrics['final_test_auc_mean'] > 0.6:
    print("ğŸ‘ Buen rendimiento! PrÃ³ximas optimizaciones:")
    print("  - Aumentar a 300 Ã©pocas")
    print("  - Probar arquitecturas mÃ¡s complejas (GraphSAGE)")
    print("  - Ajuste fino de hiperparÃ¡metros")
else:
    print("ğŸ“ˆ Rendimiento moderado. Estrategias:")
    print("  - Verificar calidad de datos")
    print("  - Probar diferentes learning rates")
    print("  - Considerar feature engineering")
